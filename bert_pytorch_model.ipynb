{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_pytorch_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKXx4CHjY06q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "43ad21da-03a7-41f3-d537-4e8509546a15"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2-u2sJLZA3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYsV4H8fCpZ-",
        "colab_type": "code",
        "outputId": "d7b6605f-33ab-45fb-aa28-7b0cfe50b8da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xjyCV7UZH-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anDT8B-NZNWE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "2fcfd385-5126-43aa-f499-a41503f8983c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsUF0tWWZY4N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "96ffe7a7-1003-4feb-ead5-7155b0f53986"
      },
      "source": [
        "cd drive/My\\ Drive/Colab\\ Notebooks/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqbQGHquZlj1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f2583e49-fe62-4ba1-be55-ee88b72a50b9"
      },
      "source": [
        "ls"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mbert\u001b[0m/                   cased_L-12_H-768_A-12.zip  \u001b[01;34mold\u001b[0m/                 sms.csv\n",
            "\u001b[01;34mcased_L-12_H-768_A-12\u001b[0m/  links.txt                  result_prepocess.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPi3JDDPZmf1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4da2f643-11cf-4460-e677-54c114088937"
      },
      "source": [
        "cd Bert_Scratch/"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Bert_Scratch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "629qizihZ-cr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caNeu0wuZvAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv('sms.csv',names=['label','text'],encoding = \"UTF-8\", quotechar='\"')\n",
        "df_train[\"text\"] = df_train['text'].str.replace('[^\\w\\s]','')\n",
        "df_train['label'] = df_train['label'].map({'ham': 0, 'spam': 1})\n",
        "df = df_train.dropna()\n",
        "df.reset_index(drop=True,inplace=True)\n",
        "df, df_test = train_test_split(df, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRJlWEqUZ8T0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = df.text.values\n",
        "labels = df.label.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b85k82YajqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z474sSC6oe7A",
        "colab_type": "code",
        "outputId": "8827f057-6139-466b-cb81-cad70e93868e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvO9IV53am5c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "8095dad2-22e4-42e1-bc3b-d963045bd6b0"
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  Was gr8 to see that message So when r u leaving Congrats dear What school and wat r ur plans\n",
            "Tokenized:  ['was', 'gr', '##8', 'to', 'see', 'that', 'message', 'so', 'when', 'r', 'u', 'leaving', 'cong', '##rat', '##s', 'dear', 'what', 'school', 'and', 'wat', 'r', 'ur', 'plans']\n",
            "Token IDs:  [2001, 24665, 2620, 2000, 2156, 2008, 4471, 2061, 2043, 1054, 1057, 2975, 26478, 8609, 2015, 6203, 2054, 2082, 1998, 28194, 1054, 24471, 3488]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bP1ha0kParsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bBdb3pt8LuQ",
        "colab_type": "code",
        "outputId": "3a96f7de-7759-452f-d49d-176dfb7b9221",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  Was gr8 to see that message So when r u leaving Congrats dear What school and wat r ur plans\n",
            "Token IDs: [101, 2001, 24665, 2620, 2000, 2156, 2008, 4471, 2061, 2043, 1054, 1057, 2975, 26478, 8609, 2015, 6203, 2054, 2082, 1998, 28194, 1054, 24471, 3488, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRAj1z7uazhu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cc1a9ff2-52ac-4069-d2bb-32915eee3db0"
      },
      "source": [
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  203\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PYW_SLYa2d9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cp9BPRd1tMIo",
        "colab_type": "code",
        "outputId": "d64bef96-5a59-4c6c-941a-356597581af7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "MAX_LEN = 64\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Padding/truncating all sentences to 64 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdstcRgAbSPf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDoC24LeEv3N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9afVoM-_bVQ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFbE-UHvsb7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use 90% for training and 10% for validation.\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=2018, test_size=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLyMNGI3bYQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw5K2A5Ko1RF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05jATZRuceF1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEgLpFVlo1Z-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7u06uXaEcwON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFsCTp_mporB",
        "colab_type": "code",
        "outputId": "a449ef19-26f1-44d8-ca28-26ad67c908ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PIiVlDYCtSq",
        "colab_type": "code",
        "outputId": "d45c8ab7-fd7a-4ac3-e90f-7a1b392c44fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        }
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (2, 768)\n",
            "classifier.bias                                                 (2,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLs72DuMODJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-p0upAhhRiIx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cQNvaZ9bnyy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AR886NhwdWX0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J-FYdx6nFE_",
        "colab_type": "code",
        "outputId": "52e6b854-b445-4755-e606-f9ad80db7a20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    125.    Elapsed: 0:00:10.\n",
            "  Batch    80  of    125.    Elapsed: 0:00:20.\n",
            "  Batch   120  of    125.    Elapsed: 0:00:30.\n",
            "\n",
            "  Average training loss: 0.12\n",
            "  Training epcoh took: 0:00:32\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    125.    Elapsed: 0:00:10.\n",
            "  Batch    80  of    125.    Elapsed: 0:00:20.\n",
            "  Batch   120  of    125.    Elapsed: 0:00:30.\n",
            "\n",
            "  Average training loss: 0.03\n",
            "  Training epcoh took: 0:00:31\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.99\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    125.    Elapsed: 0:00:10.\n",
            "  Batch    80  of    125.    Elapsed: 0:00:20.\n",
            "  Batch   120  of    125.    Elapsed: 0:00:30.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:00:31\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.99\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    125.    Elapsed: 0:00:10.\n",
            "  Batch    80  of    125.    Elapsed: 0:00:20.\n",
            "  Batch   120  of    125.    Elapsed: 0:00:30.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:31\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.99\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu9WQFXOdeHr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "4b1b23f8-fa7e-4f74-f4b2-d5986876f993"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(loss_values, 'b-o')\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvAAAAGaCAYAAABpIXfbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiIiIvIgDPBERERERB7k/wM+MHmIW4cRigAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAN0LZBOOPVh",
        "colab_type": "code",
        "outputId": "6d46727b-7580-47cf-808c-78225484a54b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df_test\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df_test.text.values\n",
        "labels = df_test.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 4,429\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hba10sXR7Xi6",
        "colab_type": "code",
        "outputId": "e7550855-b200-4e34-ab59-7c4342c78aa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 1,108 test sentences...\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5nCctJVg8sS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWcy0X1hirdx",
        "colab_type": "code",
        "outputId": "602dd074-3924-4de1-ed47-1822325374a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive samples: 604 of 4429 (13.64%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxvOo1IchChL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRaZQ4XC7kLs",
        "colab_type": "code",
        "outputId": "4ff21914-2908-4b69-be9c-9a98bf6c139b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "  \n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  \n",
        "  # Calculate and store the coef for this batch.  \n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "  matthews_set.append(matthews)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVviPdDAhGIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xytAr_C48wnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "matthews_set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qz-e0D_RhJ9p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCYZa1lQ8Jn8",
        "colab_type": "code",
        "outputId": "536894b3-e4a3-4fc4-ebf6-88934b9255a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MCC: 0.963\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ulTWaOr8QNY",
        "colab_type": "code",
        "outputId": "5c1565e1-b794-4f6e-88b3-26c2e74731e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = './model_save/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to ./model_save/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./model_save/vocab.txt',\n",
              " './model_save/special_tokens_map.json',\n",
              " './model_save/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqMzI3VTCZo5",
        "colab_type": "code",
        "outputId": "aa83de29-1f6c-47ab-9a65-d00d6ef78889",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "!ls -l --block-size=K ./model_save/"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 427948K\n",
            "-rw------- 1 root root      1K Jan 28 04:56 added_tokens.json\n",
            "-rw------- 1 root root      1K Jan 28 04:56 config.json\n",
            "-rw------- 1 root root 427719K Jan 28 04:56 pytorch_model.bin\n",
            "-rw------- 1 root root      1K Jan 28 04:56 special_tokens_map.json\n",
            "-rw------- 1 root root      1K Jan 28 04:56 tokenizer_config.json\n",
            "-rw------- 1 root root    227K Jan 28 04:56 vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WUFUIQ8Cu8D",
        "colab_type": "code",
        "outputId": "7c442925-cedd-4ede-9755-b3d533548436",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls -l --block-size=M ./model_save/pytorch_model.bin"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw------- 1 root root 418M Jan 28 04:56 ./model_save/pytorch_model.bin\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}